{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4c44fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey its working\n"
     ]
    }
   ],
   "source": [
    "print(\"Hey its working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7c865a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Samuel\\\\Desktop\\\\Investory\\\\Investory_bot'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the present working directory -> need to work fom root folder\n",
    "%pwd \n",
    "\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed2da6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samuel\\Desktop\\Investory\\Investory_bot\\invest-bot-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import orchestration... \n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133b63b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mweb_scraping\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ðŸ’¡ This reloads your script, so any changes you make in web_scraping.py are used.\u001b[39;00m\n\u001b[32m      5\u001b[39m importlib.reload(web_scraping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Samuel\\Desktop\\Investory\\Investory_bot\\web_scraping.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FPDF\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m urlparse\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import web_scraping\n",
    "\n",
    "# ðŸ’¡ This reloads your script, so any changes you make in web_scraping.py are used.\n",
    "importlib.reload(web_scraping)\n",
    "\n",
    "# List of bank websites you want to scrape\n",
    "urls_to_scrape = [\n",
    "    \"https://www.icicibank.com/personal-banking/deposits/fixed-deposit/fd-interest-rates\",\n",
    "    \"https://www.hdfcbank.com/personal/save/deposits/recurring-deposit\",\n",
    "    \"https://sbi.bank/web/personal-banking/investments-deposits/deposits/recurring-deposit\"\n",
    "]\n",
    "\n",
    "# Loop through each URL and call the scraping function\n",
    "print(\"Starting the web scraping process...\")\n",
    "for url in urls_to_scrape:\n",
    "    print(\"-\" * 20)\n",
    "    web_scraping.scrape_and_save_as_pdf(url)\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(\"Scraping complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting our content from odf files.\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data, \n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "'''  \n",
    "\n",
    "Calls the .load() method on the loader object. This method will read all the \n",
    "matching PDF files and return their contents, packaged as \"documents\" (often in LangChain, these are Document objects).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the extracted data. \n",
    "extracted_data = load_pdf_files(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ba3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff836b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering process\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, return a new list of Document objects\n",
    "    containing only 'source' in metadata and the original page_content.\n",
    "    \"\"\"\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src}\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs\n",
    "\n",
    "'''    \n",
    "3. Argument Type in a Filter Function\n",
    "The filter_to_minimal_docs function you showed expects an argument of type List[Document]:\n",
    "\n",
    "docs: List[Document] means a list (Python list) whose members are Document objects.\n",
    "The return type -> List[Document] means it gives back a list of Document objects.\n",
    "\n",
    "In Python, this type hint is just a suggestion for readers and tools; it doesnt enforce the type at runtime.\n",
    "\n",
    "\n",
    "The built-in list type is used to create and manipulate lists at runtime.\n",
    "The List from typing is used only for type hinting, to specify that a variable or parameter expects a list \n",
    "of a certain typeâ€”like List[int] for a list of integers, or List[Document] for a list of Document objects.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_data)\n",
    "minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9de751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = text_split(minimal_docs)\n",
    "print(f\"The number of chunks :  {len(text_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Vector Embedding\n",
    "\n",
    "#Loading the model. \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_embeddings() #this is the object of the embedding model, gonna use this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(\"Hi, My name is Samuel.\") # See how introducing myself would look like in a vector x0\n",
    "print(vector)\n",
    "print(len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39343ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d45764",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "\n",
    "#Returns None if the key is not found (or a default value if provided).\n",
    "# os.environ[key] raises a KeyError if the key is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f752cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing, autehnticating and making a client \n",
    "from pinecone import Pinecone\n",
    "\n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key = pinecone_api_key)\n",
    "pc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pinecone database (index)\n",
    "\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "index_name = \"invest-bot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0405b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now storing everything in PineCone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents = text_chunk,\n",
    "    embedding= embeddings,\n",
    "    index_name= index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d951cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "'''  \n",
    "Since already created, something like caching, retrive from stored rather than start whole new process. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cd512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing to see how the retrival of data is. \n",
    "\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "retrieved_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db21b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting the LLM for better readability. \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize Groq chat model\n",
    "\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ef712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember there's always 2 types of prompt. 1 -> System and 2 -> User prompt. \n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an Medical assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96488280",
   "metadata": {},
   "source": [
    "# create_stuff_documents_chain(chatModel, prompt)\n",
    "\n",
    "Creates a documents chain (called StuffDocumentsChain) that:\n",
    "\n",
    "- Takes a list of Document objects (text chunks).\n",
    "- Combines (\"stuffs\") them into a single input text block.\n",
    "- Uses the given language model (`chatModel`) to process this combined text.\n",
    "- The prompt specifies how to formulate the input for the model, typically inserting the combined text into the prompt template.\n",
    "- This chain is useful for summarization, Q&A, extraction tasks where you want the model to see the entire context at once.\n",
    "\n",
    "# create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "Creates a retrieval augmented generation (RAG) chain that:\n",
    "\n",
    "- Uses the `retriever` (which fetches relevant document chunks based on a query).\n",
    "- Passes retrieved documents to the `question_answer_chain`.\n",
    "- Returns the language model's output based on the retrieved context.\n",
    "- This enables efficient document retrieval + language modeling for tasks like open-domain Q&A, where not all documents are sent to the model but only the relevant ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ccb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acne?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is the Treatment of Acne?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09f031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c7bbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54421c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a9210ee",
   "metadata": {},
   "source": [
    "### Flow: PDF to Document Object\n",
    "\n",
    "1. **PDF File**  \n",
    "   You start with a PDF file containing raw text, images, formatting, etc.\n",
    "\n",
    "2. **Loading & Parsing**  \n",
    "   Use a PDF parser (like `PyPDFLoader` inside `DirectoryLoader`) to extract text content from the PDF.\n",
    "\n",
    "3. **Create Document Object**  \n",
    "   Each piece of extracted content (e.g., pages or chunks) is wrapped into a `Document` object. This object stores text (`page_content`) and metadata such as the document source or page number.\n",
    "\n",
    "4. **Document Processing**  \n",
    "   These `Document` objects can be passed around your pipeline (search, embedding, language model input, etc.) in a consistent format.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invest-bot-env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
